{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab5def63",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertJapaneseTokenizer,BertForSequenceClassification\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "#日本語の事前学習モデル\n",
    "MODEL_NAME = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17e10f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 41.50it/s]\n"
     ]
    }
   ],
   "source": [
    "#全記事の文章データを取得して前処理\n",
    "\n",
    "#カテゴリーのリスト\n",
    "category_list = [\n",
    "    'api',\n",
    "    'ec',\n",
    "    'movie',\n",
    "    'sns',\n",
    "    'webrtc'\n",
    "]\n",
    "\n",
    "#トークナイザのロード\n",
    "tokenizer = BertJapaneseTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "\n",
    "max_length = 128#各テキストの最大長を128トークンに設定\n",
    "dataset_for_loader = []#空のリストを初期化して、後でトークン化されたデータを格納\n",
    "\n",
    "#category_listに含まれる各カテゴリについて、インデックス(ラベル)とカテゴリ名を取得\n",
    "for label,category in enumerate(tqdm(category_list)):\n",
    "    #指定したパターンに一致するファイルのリストを取得し、それぞれのファイルを処理\n",
    "    for file in glob.glob(f'./text/{category}/{category}*'):\n",
    "        #ファイルを開いて、すべての行を読み込み\n",
    "        lines = open(file,encoding='utf-8').read().splitlines()\n",
    "        #ファイルの最初の3行を除いた残りの内容を改行で連結\n",
    "        #text = '\\n' .join(lines[3:])\n",
    "        text = '\\n' .join(lines)\n",
    "        #BERTトークナイザーを使用してテキストをトークン化\n",
    "        encoding = tokenizer(\n",
    "            text,\n",
    "            max_length=max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True\n",
    "        )\n",
    "        #トークン化されたデータにカテゴリのラベルを追加\n",
    "        encoding['labels'] = label\n",
    "        #トークナイザーの出力をPyTorchのテンソルに変換\n",
    "        encoding = {k:torch.tensor(v) for k, v in encoding.items()}\n",
    "        #前処理とラベル付けを完了したデータをリストに追加\n",
    "        dataset_for_loader.append(encoding)\n",
    "        \n",
    "        # トークンをテキストにデコードし、ラベルと一緒に表示\n",
    "        '''\n",
    "        tokens = tokenizer.convert_ids_to_tokens(encoding['input_ids'])\n",
    "        print(f\"Sample Text Tokens: {tokens[:10]}\")  # 最初の10トークンを表示\n",
    "        print(f\"Label: {label}\")\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dddbfcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#データセットをランダムにシャッフルし、サンプルの順番をランダム化\n",
    "random.shuffle(dataset_for_loader)\n",
    "#データセットの全体の長さ（サンプル数）を取得\n",
    "n = len(dataset_for_loader)\n",
    "#データセットの60%を訓練データ\n",
    "n_train = int(0.6*n)\n",
    "#データセットの20%を検証データ\n",
    "n_val = int(0.2*n)\n",
    "#計算された割合に基づいて、データセットを訓練、検証、テストデータに分割\n",
    "dataset_train = dataset_for_loader[:n_train]#学習データ\n",
    "dataset_val = dataset_for_loader[n_train:n_train+n_val]#検証データ\n",
    "dataset_test = dataset_for_loader[n_train+n_val:]#テストデータ\n",
    "\n",
    "#訓練用のデータを作成、バッチサイズの指定を行い、各エポックごとでデータの順序をランダムにする。\n",
    "dataloader_train = DataLoader(\n",
    "    dataset_train,batch_size=32,shuffle=True\n",
    ")\n",
    "#検証用とテスト用のデーターローダを作成\n",
    "datasloader_val = DataLoader(dataset_val,batch_size=64)\n",
    "dataloader_test = DataLoader(dataset_test,batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ffdb32f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#文章分類モデルの定義\n",
    "from transformers import BertForSequenceClassification, BertConfig\n",
    "class BertSewuenceClassification_pl(pl.LightningModule):\n",
    "    \n",
    "    def __init__(self,model_name,num_labels,lr):\n",
    "        '''\n",
    "        引数として、モデル名、ラベルの数、学習率を受け取る\n",
    "        '''\n",
    "        super().__init__()\n",
    "        #後からモデルのハイパーパラメータを確認\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        #BertForSequenceClassificationモデルの初期化\n",
    "        '''\n",
    "        model_name：使用するBERTの事前学習済みモデルの名前\n",
    "        num_class：分類タスクのクラス数\n",
    "        '''\n",
    "        config = BertConfig.from_pretrained(model_name, num_labels=num_labels)\n",
    "        self.bert_sc = BertForSequenceClassification.from_pretrained(model_name, config=config)\n",
    "    \n",
    "    #モデルの順伝播を定義\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
    "        output = self.bert_sc(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        return output\n",
    "    \n",
    "    #訓練時の各バッチに対する処理を定義\n",
    "    def training_step(self,batch,batch_idx):\n",
    "        '''\n",
    "        batch：データローダーから取得される1バッチ分のデータ\n",
    "        batch_idx：現在のバッチ番号\n",
    "        '''\n",
    "        #バッチ内のモデルの出力\n",
    "        output = self.bert_sc(**batch)\n",
    "        #バッチ内の損失値\n",
    "        loss = output.loss\n",
    "        self.log('train_loss',loss)\n",
    "        return loss\n",
    "    \n",
    "    #検証時の各バッチに対する処理を定義します。ここでも損失を計算し、ログに記録\n",
    "    def validation_step(self,batch,batch_idx):\n",
    "        '''\n",
    "        batch：データローダーから取得される1バッチ分のデータ\n",
    "        batch_idx：現在のバッチ番号\n",
    "        '''\n",
    "        output = self.bert_sc(**batch)\n",
    "        val_loss = output.loss\n",
    "        self.log('val_loss',val_loss)\n",
    "        \n",
    "    def test_step(self,batch,batch_idx):\n",
    "        '''\n",
    "        batch：データローダーから取得される1バッチ分のデータ\n",
    "        batch_idx：現在のバッチ番号\n",
    "        '''\n",
    "        #テストデータのバッチ全体を表す辞書から正解ラベルを取り出す\n",
    "        labels = batch.pop('labels')\n",
    "        #事前に定義された分類タスク用のBERTモデルにbatchを渡し、出力を取得\n",
    "        output = self.bert_sc(**batch)\n",
    "        #最もスコアが高いクラスのインデックスを取得\n",
    "        labels_predicted = output.logits.argmax(-1)\n",
    "        #予測されたクラスと正解ラベルを比較し、一致するかどうかをブール値で取得\n",
    "        num_correct = (labels_predicted == labels).sum().item()\n",
    "        #精度を計算\n",
    "        accuracy = num_correct/labels.size(0)\n",
    "        self.log('accuracy',accuracy)\n",
    "        \n",
    "    #モデルの最適化手法を定義、Adamオプティマイザを使用し、コンストラクタで設定された学習率を適用\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(),lr=self.hparams.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3d78b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "#トレーニング中に特定の条件（モニタリングするメトリクス）を基にモデルを保存\n",
    "checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    #モニタリングするメトリクスとして、バリデーション損失を指定\n",
    "    monitor='val_loss',\n",
    "    #モニタリングするメトリクスの最小値を基準にモデルを保存\n",
    "    mode='min',\n",
    "    #最良のモデル（val_lossが最小のモデル）1つのみを保存\n",
    "    save_top_k=1,\n",
    "    #モデルの重み（パラメータ）のみを保存\n",
    "    save_weights_only=True,\n",
    "    #保存先ディレクトリを指定\n",
    "    dirpath='model/',\n",
    ")\n",
    "\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "\n",
    "# 早期停止の設定\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor='val_loss',   # 監視する値\n",
    "    min_delta=0.00,       # 改善とみなされる最小の変化\n",
    "    patience=2,           # 改善が見られないエポック数\n",
    "    verbose=True,         # ログを出力\n",
    "    mode='min'            # 「min」は監視値の減少を目指す（損失の場合）\n",
    ")\n",
    "\n",
    "\n",
    "#Trainer クラスを使用して訓練のプロセスを設定\n",
    "trainer = pl.Trainer(\n",
    "    #使用するGPUの数を指定\n",
    "    accelerator=\"gpu\",  # GPUを使用\n",
    "    devices=1, # 使用するGPUの数 \n",
    "    max_epochs=10,# 最大エポック数\n",
    "    #トレーニング中に実行するコールバック（追加処理）のリストを指定\n",
    "    callbacks=[checkpoint, early_stop_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "825eaa99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3060 Ti') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name    | Type                          | Params | Mode\n",
      "-----------------------------------------------------------------\n",
      "0 | bert_sc | BertForSequenceClassification | 110 M  | eval\n",
      "-----------------------------------------------------------------\n",
      "110 M     Trainable params\n",
      "0         Non-trainable params\n",
      "110 M     Total params\n",
      "442.485   Total estimated model params size (MB)\n",
      "0         Modules in train mode\n",
      "231       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n",
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\pytorch_lightning\\loops\\fit_loop.py:298: The number of training batches (10) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 10/10 [00:02<00:00,  3.87it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 11.63it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  7.84it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 10/10 [00:02<00:00,  3.41it/s, v_num=0]     \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.436\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 10/10 [00:02<00:00,  4.02it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  9.17it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  7.27it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 10/10 [00:02<00:00,  3.49it/s, v_num=0]     \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.241 >= min_delta = 0.0. New best score: 1.196\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 10/10 [00:02<00:00,  3.73it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  9.00it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  7.22it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 10/10 [00:03<00:00,  3.27it/s, v_num=0]     \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.208 >= min_delta = 0.0. New best score: 0.988\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 10/10 [00:02<00:00,  4.06it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  9.10it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  7.25it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 10/10 [00:02<00:00,  3.52it/s, v_num=0]     \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.190 >= min_delta = 0.0. New best score: 0.798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 10/10 [00:02<00:00,  4.07it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 125.01it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 11.46it/s] \u001b[A\n",
      "Epoch 4: 100%|██████████| 10/10 [00:02<00:00,  3.52it/s, v_num=0]     \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.122 >= min_delta = 0.0. New best score: 0.676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 10/10 [00:02<00:00,  4.06it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  9.09it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  7.26it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 10/10 [00:02<00:00,  3.52it/s, v_num=0]     \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.098 >= min_delta = 0.0. New best score: 0.578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 10/10 [00:02<00:00,  4.07it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  9.17it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  7.28it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 10/10 [00:02<00:00,  3.53it/s, v_num=0]     \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.067 >= min_delta = 0.0. New best score: 0.511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 10/10 [00:02<00:00,  4.07it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  9.26it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  7.30it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 10/10 [00:02<00:00,  3.53it/s, v_num=0]     \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.042 >= min_delta = 0.0. New best score: 0.469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 10/10 [00:02<00:00,  4.07it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00,  9.20it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  7.28it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 10/10 [00:02<00:00,  3.53it/s, v_num=0]     \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.030 >= min_delta = 0.0. New best score: 0.440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 10/10 [00:02<00:00,  4.07it/s, v_num=0]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:  50%|█████     | 1/2 [00:00<00:00, 125.00it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 2/2 [00:00<00:00, 11.49it/s] \u001b[A\n",
      "Epoch 9: 100%|██████████| 10/10 [00:02<00:00,  3.52it/s, v_num=0]     \u001b[A"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.024 >= min_delta = 0.0. New best score: 0.415\n",
      "`Trainer.fit` stopped: `max_epochs=10` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 10/10 [00:03<00:00,  2.56it/s, v_num=0]\n"
     ]
    }
   ],
   "source": [
    "model = BertSewuenceClassification_pl(\n",
    "    MODEL_NAME,num_labels=5,lr=1e-5\n",
    ")\n",
    "\n",
    "#トレーニング開始\n",
    "trainer.fit(model,dataloader_train,datasloader_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9bcad1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ベストモデルのファイル: C:\\Users\\kinar\\Desktop\\data-preparation\\model\\epoch=9-step=100.ckpt\n",
      "ベストモデルの検証データに対する損失: tensor(0.4153, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "best_model_path = checkpoint.best_model_path#ベストモデルのファイル\n",
    "print('ベストモデルのファイル:',checkpoint.best_model_path)\n",
    "print('ベストモデルの検証データに対する損失:',checkpoint.best_model_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1cdc8b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "C:\\Users\\kinar\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=19` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  6.25it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">        Test metric        </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         accuracy          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.9100000262260437     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m        accuracy         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.9100000262260437    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.91\n"
     ]
    }
   ],
   "source": [
    "# テストデータでの評価\n",
    "test_result = trainer.test(model, dataloaders=dataloader_test)\n",
    "print(f\"Accuracy on test data: {test_result[0]['accuracy']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "280ce028",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at cl-tohoku/bert-base-japanese-whole-word-masking and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#PyTorch Lightningモデルのロード\n",
    "model = BertSewuenceClassification_pl.load_from_checkpoint(\n",
    "    best_model_path\n",
    ")\n",
    "\n",
    "#Transformers対応モデルを./model_transformersに保存\n",
    "model.bert_sc.save_pretrained('./model_transformers')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f6909c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_sc = BertForSequenceClassification.from_pretrained(\n",
    "    './model_transformers/'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5279ce3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_sc.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4f741c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分析したいテキスト\n",
    "text = \"テキストで、コミュニケーションを行えるサービスを開発したいです。\"\n",
    "\n",
    "# テキストをトークナイズしてテンソルに変換\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db3ff346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted class ID: 3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 推論を実行（勾配計算は不要）\n",
    "with torch.no_grad():\n",
    "    outputs = bert_sc(**inputs)\n",
    "\n",
    "# ロジットから予測されたクラスIDを取得\n",
    "predicted_class_id = outputs.logits.argmax(-1).item()\n",
    "print(f\"Predicted class ID: {predicted_class_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2a854411",
   "metadata": {},
   "outputs": [
    {
     "ename": "NoSuchFile",
     "evalue": "[ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from bert_model.onnx failed:Load model bert_model.onnx failed. File doesn't exist",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNoSuchFile\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# ONNX Runtime セッションの作成\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m session \u001b[38;5;241m=\u001b[39m ort\u001b[38;5;241m.\u001b[39mInferenceSession(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbert_model.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# モデルの入力名と出力名を取得\u001b[39;00m\n\u001b[0;32m      8\u001b[0m input_name \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mget_inputs()[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mname\n",
      "File \u001b[1;32m~\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:465\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[1;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[0;32m    462\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_inference_session(providers, provider_options, disabled_optimizers)\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[1;32m~\\.conda\\envs\\natural_langugage_processing\\Lib\\site-packages\\onnxruntime\\capi\\onnxruntime_inference_collection.py:526\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[1;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[0;32m    523\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_register_ep_custom_ops(session_options, providers, provider_options, available_providers)\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path:\n\u001b[1;32m--> 526\u001b[0m     sess \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mInferenceSession(session_options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_path, \u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_config_from_model)\n\u001b[0;32m    527\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    528\u001b[0m     sess \u001b[38;5;241m=\u001b[39m C\u001b[38;5;241m.\u001b[39mInferenceSession(session_options, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_bytes, \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_read_config_from_model)\n",
      "\u001b[1;31mNoSuchFile\u001b[0m: [ONNXRuntimeError] : 3 : NO_SUCHFILE : Load model from bert_model.onnx failed:Load model bert_model.onnx failed. File doesn't exist"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a5e78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
